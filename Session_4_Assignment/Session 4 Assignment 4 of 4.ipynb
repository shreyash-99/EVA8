{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyash-99/EVA8/blob/main/Session%204%20Assignment%204%20of%20n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Target \n",
        "1. Firstly implement stepLR to get consistent results\n",
        "2. Reduce the size by a little bit to make it under 10k by changing some channel sizes\n",
        "3. Will also try to increase to initial lr."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFn93Z6zxHJ2"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "67SgeRG8xNmF"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([transforms.RandomRotation((-7.0,7.0), fill = (1,)) ,  \n",
        "                                      transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "test_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.1307,), (0.3081, ))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UlkNtVM734g4"
      },
      "outputs": [],
      "source": [
        "train = datasets.MNIST('./data', train = True, download = True, transform = train_transforms)\n",
        "test = datasets.MNIST('./data', train = False, download = True, transform = test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU1WhqE136y2",
        "outputId": "c19f69a0-731c-438d-884e-226444e703d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "SEED = 1\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA available\" , cuda)\n",
        "\n",
        "#for Reproducibilty\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# setting the dataloader arguments for both test and train data\n",
        "dataloader_args = dict(shuffle = True, batch_size = 128, num_workers = 4, pin_memory = True) if cuda else dict(shuffle = True, batch_size = 64)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train , **dataloader_args)\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9vaS64ac39QO"
      },
      "outputs": [],
      "source": [
        "dropout_value = 0.07\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    ## INPUT BLOCK (BLOCK  1 )\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3), bias = False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_value)   \n",
        "    )# input - 28, output - 26\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), padding = 1, bias = False),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_value)\n",
        "    )#input - 26 , output - 26\n",
        "    \n",
        "\n",
        "    # TRANSITION BLOCK \n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 16, out_channels = 8, kernel_size = (1,1), bias = False),\n",
        "        # nn.BatchNorm2d(10),\n",
        "        # nn.ReLU(),\n",
        "        # nn.Dropout(dropout_value)\n",
        "    )# input - 26 , output - 26\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    # input 26, output - 13\n",
        "\n",
        "    # CONVOLUTION  BLOCK 2\n",
        "    self.conv4 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 8, out_channels = 16 , kernel_size = (3,3), bias = False),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),#input = 13 , output  = 11\n",
        "        nn.Dropout(dropout_value)\n",
        "    )\n",
        "    self.conv5 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = (3,3), padding = 1, bias = False),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),# input - 11 , output = 11\n",
        "        nn.Dropout(dropout_value)\n",
        "    )\n",
        "\n",
        "    #   TRANSITION BLOCK \n",
        "    self.conv6 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 16, out_channels = 8, kernel_size = (1,1), bias = False),\n",
        "        # nn.BatchNorm2d(10),\n",
        "        # nn.ReLU(), # input - 11, output - 11\n",
        "        # nn.Dropout(dropout_value)\n",
        "    )\n",
        "    # # self.pool2 = nn.MaxPool2d(2,2) #input 12, output 6\n",
        "\n",
        "    # #   CONVOLUTION BLOCK 3\n",
        "\n",
        "    self.conv7 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 8, out_channels = 8 , kernel_size = (3,3),bias = False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU(),#input = 11, output = 9\n",
        "        nn.Dropout(dropout_value)\n",
        "    )\n",
        "    self.conv8 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels= 8, out_channels = 16, kernel_size = (3,3), bias = False),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),#input = 9, output = 7\n",
        "        nn.Dropout(dropout_value)\n",
        "    )\n",
        "    self.conv9 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = (3,3), bias = False),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_value) \n",
        "    )#input - 7 , output - 5 \n",
        "\n",
        "\n",
        "    #      CONVOLUTION BLOCK 4\n",
        "    self.gap = nn.Sequential(\n",
        "        nn.AvgPool2d(kernel_size = 5)\n",
        "    )# output 1\n",
        "    self.conv10 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 16, out_channels = 10, kernel_size = (1,1), bias = False)\n",
        "    )\n",
        "    \n",
        "\n",
        "\n",
        "  def forward(self, x ):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.conv6(x)\n",
        "\n",
        "    x = self.conv7(x)\n",
        "    x = self.conv8(x)\n",
        "\n",
        "    x = self.conv9(x)\n",
        "\n",
        "    x = self.gap(x)\n",
        "\n",
        "    x = self.conv10(x)\n",
        "\n",
        "\n",
        "    x = x.view(-1,10)\n",
        "    return F.log_softmax(x, dim = -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odnp0mo84GWI",
        "outputId": "56e8df4b-6055-483b-a42f-5dc85b8e0691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              72\n",
            "       BatchNorm2d-2            [-1, 8, 26, 26]              16\n",
            "              ReLU-3            [-1, 8, 26, 26]               0\n",
            "           Dropout-4            [-1, 8, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 26, 26]           1,152\n",
            "       BatchNorm2d-6           [-1, 16, 26, 26]              32\n",
            "              ReLU-7           [-1, 16, 26, 26]               0\n",
            "           Dropout-8           [-1, 16, 26, 26]               0\n",
            "            Conv2d-9            [-1, 8, 26, 26]             128\n",
            "        MaxPool2d-10            [-1, 8, 13, 13]               0\n",
            "           Conv2d-11           [-1, 16, 11, 11]           1,152\n",
            "      BatchNorm2d-12           [-1, 16, 11, 11]              32\n",
            "             ReLU-13           [-1, 16, 11, 11]               0\n",
            "          Dropout-14           [-1, 16, 11, 11]               0\n",
            "           Conv2d-15           [-1, 16, 11, 11]           2,304\n",
            "      BatchNorm2d-16           [-1, 16, 11, 11]              32\n",
            "             ReLU-17           [-1, 16, 11, 11]               0\n",
            "          Dropout-18           [-1, 16, 11, 11]               0\n",
            "           Conv2d-19            [-1, 8, 11, 11]             128\n",
            "           Conv2d-20              [-1, 8, 9, 9]             576\n",
            "      BatchNorm2d-21              [-1, 8, 9, 9]              16\n",
            "             ReLU-22              [-1, 8, 9, 9]               0\n",
            "          Dropout-23              [-1, 8, 9, 9]               0\n",
            "           Conv2d-24             [-1, 16, 7, 7]           1,152\n",
            "      BatchNorm2d-25             [-1, 16, 7, 7]              32\n",
            "             ReLU-26             [-1, 16, 7, 7]               0\n",
            "          Dropout-27             [-1, 16, 7, 7]               0\n",
            "           Conv2d-28             [-1, 16, 5, 5]           2,304\n",
            "      BatchNorm2d-29             [-1, 16, 5, 5]              32\n",
            "             ReLU-30             [-1, 16, 5, 5]               0\n",
            "          Dropout-31             [-1, 16, 5, 5]               0\n",
            "        AvgPool2d-32             [-1, 16, 1, 1]               0\n",
            "           Conv2d-33             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 9,320\n",
            "Trainable params: 9,320\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.73\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size = (1,28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0jPFP12_4JDN"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimiser, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    #get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    #init\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    #prediction\n",
        "    y_pred = model(data)\n",
        "\n",
        "    #calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    #Backpropagation\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    # update pbar - tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim = 1, keepdim = True) # gets the index of the max log-probabilirty\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "      pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  \n",
        "  test_acc.append(100. * correct / len(test_loader.dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF36if9J4N4L",
        "outputId": "6ec1026f-a899-4ecf-829e-fe36f76a8441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0 Learning Rate:  [0.03]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.13147200644016266 Batch_id=468 Accuracy=89.32: 100%|██████████| 469/469 [00:18<00:00, 25.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0672, Accuracy: 9784/10000 (97.84%)\n",
            "\n",
            "EPOCH: 1 Learning Rate:  [0.03]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.026439359411597252 Batch_id=468 Accuracy=97.81: 100%|██████████| 469/469 [00:17<00:00, 26.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0539, Accuracy: 9836/10000 (98.36%)\n",
            "\n",
            "EPOCH: 2 Learning Rate:  [0.03]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.10534457117319107 Batch_id=468 Accuracy=98.23: 100%|██████████| 469/469 [00:18<00:00, 25.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0330, Accuracy: 9894/10000 (98.94%)\n",
            "\n",
            "EPOCH: 3 Learning Rate:  [0.03]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.03889813646674156 Batch_id=468 Accuracy=98.47: 100%|██████████| 469/469 [00:18<00:00, 25.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0258, Accuracy: 9913/10000 (99.13%)\n",
            "\n",
            "EPOCH: 4 Learning Rate:  [0.03]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.18354378640651703 Batch_id=468 Accuracy=98.47: 100%|██████████| 469/469 [00:18<00:00, 25.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0323, Accuracy: 9893/10000 (98.93%)\n",
            "\n",
            "EPOCH: 5 Learning Rate:  [0.03]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.023572811856865883 Batch_id=468 Accuracy=98.69: 100%|██████████| 469/469 [00:19<00:00, 24.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0316, Accuracy: 9906/10000 (99.06%)\n",
            "\n",
            "EPOCH: 6 Learning Rate:  [0.003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.017485691234469414 Batch_id=468 Accuracy=98.97: 100%|██████████| 469/469 [00:17<00:00, 26.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "EPOCH: 7 Learning Rate:  [0.003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.013596788048744202 Batch_id=468 Accuracy=99.04: 100%|██████████| 469/469 [00:17<00:00, 26.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 9945/10000 (99.45%)\n",
            "\n",
            "EPOCH: 8 Learning Rate:  [0.003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.07576064765453339 Batch_id=468 Accuracy=99.02: 100%|██████████| 469/469 [00:18<00:00, 25.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0200, Accuracy: 9947/10000 (99.47%)\n",
            "\n",
            "EPOCH: 9 Learning Rate:  [0.003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.09534329921007156 Batch_id=468 Accuracy=99.03: 100%|██████████| 469/469 [00:17<00:00, 26.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0195, Accuracy: 9945/10000 (99.45%)\n",
            "\n",
            "EPOCH: 10 Learning Rate:  [0.003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0043024662882089615 Batch_id=468 Accuracy=99.07: 100%|██████████| 469/469 [00:17<00:00, 27.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0183, Accuracy: 9950/10000 (99.50%)\n",
            "\n",
            "EPOCH: 11 Learning Rate:  [0.003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0062692477367818356 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:17<00:00, 26.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 12 Learning Rate:  [0.00030000000000000003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.01717514358460903 Batch_id=468 Accuracy=99.12: 100%|██████████| 469/469 [00:17<00:00, 26.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0189, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 13 Learning Rate:  [0.00030000000000000003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.002918594516813755 Batch_id=468 Accuracy=99.11: 100%|██████████| 469/469 [00:17<00:00, 27.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 14 Learning Rate:  [0.00030000000000000003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.006048992276191711 Batch_id=468 Accuracy=99.21: 100%|██████████| 469/469 [00:18<00:00, 25.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 9949/10000 (99.49%)\n",
            "\n",
            "EPOCH: 15 Learning Rate:  [0.00030000000000000003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0249591376632452 Batch_id=468 Accuracy=99.14: 100%|██████████| 469/469 [00:17<00:00, 26.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 16 Learning Rate:  [0.00030000000000000003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.033999521285295486 Batch_id=468 Accuracy=99.16: 100%|██████████| 469/469 [00:17<00:00, 26.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0189, Accuracy: 9948/10000 (99.48%)\n",
            "\n",
            "EPOCH: 17 Learning Rate:  [0.00030000000000000003]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.03355110064148903 Batch_id=468 Accuracy=99.13: 100%|██████████| 469/469 [00:18<00:00, 25.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 9949/10000 (99.49%)\n",
            "\n",
            "EPOCH: 18 Learning Rate:  [3.0000000000000004e-05]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.11394136399030685 Batch_id=468 Accuracy=99.14: 100%|██████████| 469/469 [00:18<00:00, 25.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0184, Accuracy: 9947/10000 (99.47%)\n",
            "\n",
            "EPOCH: 19 Learning Rate:  [3.0000000000000004e-05]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0030106117483228445 Batch_id=468 Accuracy=99.11: 100%|██████████| 469/469 [00:17<00:00, 26.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 9947/10000 (99.47%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch, \"Learning Rate: \", scheduler.get_last_lr())\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results:\n",
        "1. Parameters: 9.3k\n",
        "2. Best Test Accuracy: 99.50% (till 15th epoch)\n",
        "3. Best Train Accuracy: 99.21% (till 15th epoch)\n",
        "\n",
        "Conclusion:\n",
        "1. Model is very good as it is not at all overfitting, results are good it achieves 99.4% accuracy from 7th epoch \n",
        "2. Dont know how reducing the but reducing the channel the size of channels(16->8) for first was better for the model.\n",
        "3. Increasing the lr to 0.3 also helped to achieve the target fast and steplr helped it to reduced it so that the target doesnt overshoot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1aZflwE4h66"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNaQsZmZyQOJp6Ef2DT/TkC",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
