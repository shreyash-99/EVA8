{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n",
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n",
      "creating dataset...\n",
      "initializing model...\n",
      "initializing optimizer and loss...\n",
      "training...\n",
      "it: 0  | loss 10.19  | Δw: 1.073\n",
      "it: 10  | loss 9.52  | Δw: 0.524\n",
      "it: 20  | loss 9.28  | Δw: 0.34\n",
      "it: 30  | loss 9.13  | Δw: 0.271\n",
      "it: 40  | loss 8.93  | Δw: 0.224\n",
      "it: 50  | loss 8.78  | Δw: 0.207\n",
      "it: 60  | loss 8.62  | Δw: 0.191\n",
      "it: 70  | loss 8.54  | Δw: 0.185\n",
      "it: 80  | loss 8.33  | Δw: 0.169\n",
      "it: 90  | loss 8.18  | Δw: 0.163\n",
      "it: 100  | loss 8.09  | Δw: 0.155\n",
      "it: 110  | loss 7.87  | Δw: 0.153\n",
      "it: 120  | loss 7.81  | Δw: 0.147\n",
      "it: 130  | loss 7.62  | Δw: 0.138\n",
      "it: 140  | loss 7.48  | Δw: 0.133\n",
      "it: 150  | loss 7.47  | Δw: 0.133\n",
      "it: 160  | loss 7.27  | Δw: 0.133\n",
      "it: 170  | loss 7.2  | Δw: 0.128\n",
      "it: 180  | loss 7.15  | Δw: 0.129\n",
      "it: 190  | loss 7.05  | Δw: 0.124\n",
      "it: 200  | loss 6.9  | Δw: 0.123\n",
      "it: 210  | loss 6.89  | Δw: 0.123\n",
      "it: 220  | loss 6.88  | Δw: 0.129\n",
      "it: 230  | loss 6.77  | Δw: 0.121\n",
      "it: 240  | loss 6.69  | Δw: 0.126\n",
      "it: 250  | loss 6.71  | Δw: 0.123\n",
      "it: 260  | loss 6.58  | Δw: 0.123\n",
      "it: 270  | loss 6.58  | Δw: 0.13\n",
      "it: 280  | loss 6.59  | Δw: 0.127\n",
      "it: 290  | loss 6.53  | Δw: 0.124\n",
      "it: 300  | loss 6.52  | Δw: 0.133\n",
      "it: 310  | loss 6.55  | Δw: 0.143\n",
      "it: 320  | loss 6.43  | Δw: 0.142\n",
      "it: 330  | loss 6.43  | Δw: 0.142\n",
      "it: 340  | loss 6.49  | Δw: 0.151\n",
      "it: 350  | loss 6.46  | Δw: 0.2\n",
      "it: 360  | loss 6.43  | Δw: 0.176\n",
      "it: 370  | loss 6.37  | Δw: 0.2\n",
      "it: 380  | loss 6.39  | Δw: 0.221\n",
      "it: 390  | loss 6.31  | Δw: 0.236\n",
      "it: 400  | loss 6.42  | Δw: 0.246\n",
      "it: 410  | loss 6.37  | Δw: 0.293\n",
      "it: 420  | loss 6.34  | Δw: 0.31\n",
      "it: 430  | loss 6.4  | Δw: 0.333\n",
      "it: 440  | loss 6.42  | Δw: 0.362\n",
      "it: 450  | loss 6.41  | Δw: 0.393\n",
      "it: 460  | loss 6.31  | Δw: 0.414\n",
      "it: 470  | loss 6.34  | Δw: 0.437\n",
      "it: 480  | loss 6.26  | Δw: 0.481\n",
      "it: 490  | loss 6.32  | Δw: 0.531\n",
      "it: 500  | loss 6.36  | Δw: 0.478\n",
      "it: 510  | loss 6.34  | Δw: 0.504\n",
      "it: 520  | loss 6.26  | Δw: 0.585\n",
      "it: 530  | loss 6.27  | Δw: 0.606\n",
      "it: 540  | loss 6.33  | Δw: 0.609\n",
      "it: 550  | loss 6.32  | Δw: 0.576\n",
      "it: 560  | loss 6.31  | Δw: 0.597\n",
      "it: 570  | loss 6.36  | Δw: 0.641\n",
      "it: 580  | loss 6.27  | Δw: 0.686\n",
      "it: 590  | loss 6.2  | Δw: 0.777\n",
      "it: 600  | loss 6.22  | Δw: 0.809\n",
      "it: 610  | loss 6.26  | Δw: 0.841\n",
      "it: 620  | loss 6.23  | Δw: 0.786\n",
      "it: 630  | loss 6.22  | Δw: 0.793\n",
      "it: 640  | loss 6.28  | Δw: 0.816\n",
      "it: 650  | loss 6.31  | Δw: 0.837\n",
      "it: 660  | loss 6.33  | Δw: 0.859\n",
      "it: 670  | loss 6.24  | Δw: 0.904\n",
      "it: 680  | loss 6.21  | Δw: 0.925\n",
      "it: 690  | loss 6.26  | Δw: 0.985\n",
      "it: 700  | loss 6.26  | Δw: 1.0\n",
      "it: 710  | loss 6.19  | Δw: 1.022\n",
      "it: 720  | loss 6.19  | Δw: 0.957\n",
      "it: 730  | loss 6.2  | Δw: 1.155\n",
      "it: 740  | loss 6.21  | Δw: 1.134\n",
      "it: 750  | loss 6.25  | Δw: 0.987\n",
      "it: 760  | loss 6.12  | Δw: 1.14\n",
      "it: 770  | loss 6.2  | Δw: 1.108\n",
      "it: 780  | loss 6.13  | Δw: 1.114\n",
      "it: 790  | loss 6.28  | Δw: 1.168\n",
      "it: 800  | loss 6.19  | Δw: 1.282\n",
      "it: 810  | loss 6.21  | Δw: 1.263\n",
      "it: 820  | loss 6.21  | Δw: 1.287\n",
      "it: 830  | loss 6.14  | Δw: 1.283\n",
      "it: 840  | loss 6.09  | Δw: 1.263\n",
      "it: 850  | loss 6.19  | Δw: 1.25\n",
      "it: 860  | loss 6.13  | Δw: 1.294\n",
      "it: 870  | loss 6.3  | Δw: 1.467\n",
      "it: 880  | loss 6.13  | Δw: 1.333\n",
      "it: 890  | loss 6.21  | Δw: 1.407\n",
      "it: 900  | loss 6.21  | Δw: 1.342\n",
      "it: 910  | loss 6.18  | Δw: 1.369\n",
      "it: 920  | loss 6.19  | Δw: 1.38\n",
      "it: 930  | loss 6.16  | Δw: 1.506\n",
      "it: 940  | loss 6.18  | Δw: 1.44\n",
      "it: 950  | loss 6.09  | Δw: 1.523\n",
      "it: 960  | loss 6.11  | Δw: 1.502\n",
      "it: 970  | loss 6.16  | Δw: 1.629\n",
      "it: 980  | loss 6.09  | Δw: 1.494\n",
      "it: 990  | loss 6.18  | Δw: 1.569\n",
      "saving embeddings...\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Libs\n",
    "# =============================================================================\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "\n",
    "# =============================================================================\n",
    "# Transformer\n",
    "# =============================================================================\n",
    "def attention(q, k, v, mask = None, dropout = None):\n",
    "    scores = q.matmul(k.transpose(-2, -1))\n",
    "    scores /= math.sqrt(q.shape[-1])\n",
    "    \n",
    "    #mask\n",
    "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)## if there is a mask, this replaces the score of masked element with a large negative value so that these done affect future calculations much\n",
    "    \n",
    "    scores = F.softmax(scores, dim = -1)\n",
    "    scores = dropout(scores) if dropout is not None else scores\n",
    "    output = scores.matmul(v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
    "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.out_dim = out_dim\n",
    "        self.out_dim_per_head = out_dim // n_heads\n",
    "        self.out = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, t):\n",
    "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
    "    \n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        #in decoder, y comes from encoder. In encoder, y=x\n",
    "        y = x if y is None else y\n",
    "        \n",
    "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
    "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        \n",
    "        #break into n_heads\n",
    "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
    "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        \n",
    "        #n_heads => attention => merge the heads => mix information\n",
    "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD  ## all the calculation happen with keeping different heads in mind\n",
    "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
    "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #inp => inner => relu => dropout => inner => inp\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
    "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
    "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #model input\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
    "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
    "        \n",
    "        #backbone\n",
    "        encoders = []\n",
    "        for i in range(n_code):\n",
    "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        \n",
    "        #language model\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x + self.pe(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Positional Embedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80): ##d_model - dimension of the each embedding of a work ## max_seq_length - legth of a sentence\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe.requires_grad = False ## as the values are stores as exact and cant be baackpropagated to change them acc to nn.\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2): ## the 2 means that it iterates over the even indices of the embedding.\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model))) ## first iterating ov er every word and then iterating over each embedding of that word and then adding pos emb. to that emd acc to the place of the word and that embedding\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model))) ## adding positional embedding to a matrix will all terms = 0, cant be modified and remains same always.\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len so that the size of the seq_length is same as that of the embedding so that adding can occur directly\n",
    "    \n",
    "    \n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "class SentencesDataset(Dataset):\n",
    "    #Init dataset\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        dataset = self\n",
    "        \n",
    "        dataset.sentences = sentences\n",
    "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
    "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
    "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
    "        dataset.seq_len = seq_len\n",
    "        \n",
    "        #special tags\n",
    "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
    "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
    "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
    "    \n",
    "    \n",
    "    #fetch data\n",
    "    def __getitem__(self, index, p_random_mask=0.15):\n",
    "        dataset = self\n",
    "        \n",
    "        #while we don't have enough word to fill the sentence for a batch\n",
    "        s = []\n",
    "        while len(s) < dataset.seq_len:\n",
    "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
    "            index += 1\n",
    "        \n",
    "        #ensure that the sequence is of length seq_len\n",
    "        s = s[:dataset.seq_len]\n",
    "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
    "        \n",
    "        #apply random mask\n",
    "        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
    "        \n",
    "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
    "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
    "\n",
    "    #return length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #get words id\n",
    "    def get_sentence_idx(self, index):\n",
    "        dataset = self\n",
    "        s = dataset.sentences[index]\n",
    "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
    "        return s\n",
    "\n",
    "# =============================================================================\n",
    "# Methods / Class\n",
    "# =============================================================================\n",
    "def get_batch(loader, loader_iter):\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    return batch, loader_iter\n",
    "\n",
    "# =============================================================================\n",
    "# #Init\n",
    "# =============================================================================\n",
    "print('initializing..')\n",
    "batch_size = 1024\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n",
    "\n",
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'training.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "#init model\n",
    "print('initializing model...')\n",
    "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "model = model.cuda()\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
    "\n",
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 10\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 1000\n",
    "for it in range(n_iteration):\n",
    "    \n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "    \n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "    \n",
    "    masked_input = masked_input.cuda(non_blocking=True)\n",
    "    masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output = model(masked_input)\n",
    "    \n",
    "    #compute the cross entropy loss \n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it, \n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# Results analysis\n",
    "# =============================================================================\n",
    "print('saving embeddings...')\n",
    "N = 3000\n",
    "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
    "s = [dataset.rvocab[i] for i in range(N)]\n",
    "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
    "\n",
    "\n",
    "print('end')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get a random index from a dictionary\n",
    "def get_random_index(dictionary):\n",
    "    return random.choice(list(dictionary.keys()))       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
